{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerias\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'THE DIARY OF A YOUNG GIRL'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importando datos                                                        \n",
    "with open('annafrank.txt') as f:\n",
    "    data = f.read()\n",
    "data[0:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/macbookpro/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Descargando recursos semanticos\n",
    "nltk.download('stopwords')\n",
    "sw = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['diary',\n",
       " 'young',\n",
       " 'girl',\n",
       " 'definitive',\n",
       " 'edition',\n",
       " 'anne',\n",
       " 'frank',\n",
       " 'edited',\n",
       " 'otto',\n",
       " 'frank',\n",
       " 'mirjam',\n",
       " 'pressler',\n",
       " 'translated',\n",
       " 'susan',\n",
       " 'massotty',\n",
       " 'book',\n",
       " 'flap',\n",
       " 'anne',\n",
       " 'frank',\n",
       " 'diary']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocesamiento\n",
    "data = data.lower() # Conviertiendo texto en minuscula\n",
    "data = re.sub(r'[,!?;-]', '.',data) # Eliminando caracteres raros\n",
    "data = word_tokenize(data)\n",
    "data = [i for i in data if i.isalpha() or i == '.'] # Eliminando caracter no alfanumericos\n",
    "data = [i for i in data if i not in sw]\n",
    "data = [lemmatizer.lemmatize(i) for i in data]\n",
    "data[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posterior al preprocesamiento, se debe crear el vocabulario de las palabras unicas contenidas dentro del texto. Tal como se muestra a continuacion, el texto cuenta con 7087 palabras unicas ordenadas alfabeticamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7087\n",
      "['.', 'aachen', 'aagje', 'aah', 'abandon', 'abandoned', 'abduction', 'aber', 'abide', 'ability', 'abject', 'ablaze', 'able', 'abominable', 'abounds', 'aboveboard', 'abruptly', 'absent', 'absentminded', 'absolute']\n"
     ]
    }
   ],
   "source": [
    "# Se crea el vocabulario de palabras\n",
    "voc = set(data)\n",
    "voc = sorted(voc)\n",
    "print(len(voc))\n",
    "print(voc[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hablar de :\n",
    "diferentes metodos de encoding a nivel general (cuales no se van a usar)\n",
    "datos a utilizar\n",
    "preprocesamiento\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot Encoder\n",
    "El One Hot Encoder es una evolucion del indicator encoding. Este ultimo codifica cada palabra en una nueva variable hasta obtener N-1 columnas, siendo N el tama√±o del vocabulario. El indicator encoding eliminaba una columna dado que las categorias son mutuamente excluyentes, por lo tanto el tener N variables podria romper el supuesto de independencia entre las palabras que se quieren modelar. Actualmente, esto no es necesario dado que que los modelos modernos generalmente incorporan metodos de regularizacion, disminuyendo la importancia de variables linealmente correlaccionadas.\n",
    "\n",
    "A partir de lo anterior, surge el one Hot Encoder el cual es uno de los mas usados entre los metodos de conteo, el cual crea un vector para cada palabra y dentro de ese vector se asigna el valor de 1 para representar la presencia de la palabra dentro de una posicion especifica del vocabulario\n",
    "\n",
    "$$abandono = [0, 0, 0, 0, 1, 0, 0 ..... 0, 0, 0, 0]_{300}$$\n",
    "\n",
    "Para el ejemplo anterior se muestra la representacion vectorial de la palabra **abandono**, la cual tiene un valor de 1 en la posicion 4 (tomando como valor de inicio el 0). Es decir, dentro del vocabulario, **abandono** se encuentra en la posicion 4. Asimimo, cuando se agregan todos los vectores de las palabras del vocabulario se obtiene una matriz tal y como se muestra a continuacion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0_.</th>\n",
       "      <th>x0_aachen</th>\n",
       "      <th>x0_aagje</th>\n",
       "      <th>x0_aah</th>\n",
       "      <th>x0_abandon</th>\n",
       "      <th>x0_abandoned</th>\n",
       "      <th>x0_abduction</th>\n",
       "      <th>x0_aber</th>\n",
       "      <th>x0_abide</th>\n",
       "      <th>x0_ability</th>\n",
       "      <th>...</th>\n",
       "      <th>x0_zealand</th>\n",
       "      <th>x0_zero</th>\n",
       "      <th>x0_zeus</th>\n",
       "      <th>x0_zhlobin</th>\n",
       "      <th>x0_zionist</th>\n",
       "      <th>x0_zipper</th>\n",
       "      <th>x0_zone</th>\n",
       "      <th>x0_zookeeper</th>\n",
       "      <th>x0_zu</th>\n",
       "      <th>x0_zweite</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 7087 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   x0_.  x0_aachen  x0_aagje  x0_aah  x0_abandon  x0_abandoned  x0_abduction  \\\n",
       "0   1.0        0.0       0.0     0.0         0.0           0.0           0.0   \n",
       "1   0.0        1.0       0.0     0.0         0.0           0.0           0.0   \n",
       "2   0.0        0.0       1.0     0.0         0.0           0.0           0.0   \n",
       "3   0.0        0.0       0.0     1.0         0.0           0.0           0.0   \n",
       "4   0.0        0.0       0.0     0.0         1.0           0.0           0.0   \n",
       "\n",
       "   x0_aber  x0_abide  x0_ability  ...  x0_zealand  x0_zero  x0_zeus  \\\n",
       "0      0.0       0.0         0.0  ...         0.0      0.0      0.0   \n",
       "1      0.0       0.0         0.0  ...         0.0      0.0      0.0   \n",
       "2      0.0       0.0         0.0  ...         0.0      0.0      0.0   \n",
       "3      0.0       0.0         0.0  ...         0.0      0.0      0.0   \n",
       "4      0.0       0.0         0.0  ...         0.0      0.0      0.0   \n",
       "\n",
       "   x0_zhlobin  x0_zionist  x0_zipper  x0_zone  x0_zookeeper  x0_zu  x0_zweite  \n",
       "0         0.0         0.0        0.0      0.0           0.0    0.0        0.0  \n",
       "1         0.0         0.0        0.0      0.0           0.0    0.0        0.0  \n",
       "2         0.0         0.0        0.0      0.0           0.0    0.0        0.0  \n",
       "3         0.0         0.0        0.0      0.0           0.0    0.0        0.0  \n",
       "4         0.0         0.0        0.0      0.0           0.0    0.0        0.0  \n",
       "\n",
       "[5 rows x 7087 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aplicando OHE\n",
    "OHE = OneHotEncoder()\n",
    "OHE.fit(np.reshape(voc, (-1, 1)))\n",
    "embeddingOHE = pd.DataFrame(OHE.transform(np.reshape(voc, (-1, 1))).toarray(), columns=OHE.get_feature_names_out())\n",
    "embeddingOHE.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tal como se muestra en la tabla, se crea una matriz simetrica de 7087x7087. El hecho de que sea simetrica indica que el vector que representa cada palabra puede obtenerse al extraer una fila o columna de la matriz. Por ejemplo, para la palabra abandono se puede extraer el vector para ambos componentes y compararlos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector por columnas: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "Vector por filas: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Extrayendo 10 primeros elementos vector palabra abandon\n",
    "print(\"Vector por columnas:\", np.array(embeddingOHE['x0_abandon'])[:10])\n",
    "print(\"Vector por filas:\", np.array(embeddingOHE.iloc[4, :])[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debilidades en la similaridad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
