{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerias\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'THE DIARY OF A YOUNG GIRL'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importando datos                                                        \n",
    "with open('annafrank.txt') as f:\n",
    "    data = f.read()\n",
    "data[0:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/macbookpro/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Descargando recursos semanticos\n",
    "nltk.download('stopwords')\n",
    "sw = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['diary',\n",
       " 'young',\n",
       " 'girl',\n",
       " 'definitive',\n",
       " 'edition',\n",
       " 'anne',\n",
       " 'frank',\n",
       " 'edited',\n",
       " 'otto',\n",
       " 'frank',\n",
       " 'mirjam',\n",
       " 'pressler',\n",
       " 'translated',\n",
       " 'susan',\n",
       " 'massotty',\n",
       " 'book',\n",
       " 'flap',\n",
       " 'anne',\n",
       " 'frank',\n",
       " 'diary']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocesamiento\n",
    "data = data.lower() # Conviertiendo texto en minuscula\n",
    "data = re.sub(r'[,!?;-]', '.',data) # Eliminando caracteres raros\n",
    "data = word_tokenize(data)\n",
    "data = [i for i in data if i.isalpha() or i == '.'] # Eliminando caracter no alfanumericos\n",
    "data = [i for i in data if i not in sw]\n",
    "data = [lemmatizer.lemmatize(i) for i in data]\n",
    "# Sinonimos\n",
    "data[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posterior al preprocesamiento, se debe crear el vocabulario de las palabras unicas contenidas dentro del texto. Tal como se muestra a continuacion, el texto cuenta con 7087 palabras unicas ordenadas alfabeticamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7087\n",
      "['.', 'aachen', 'aagje', 'aah', 'abandon', 'abandoned', 'abduction', 'aber', 'abide', 'ability', 'abject', 'ablaze', 'able', 'abominable', 'abounds', 'aboveboard', 'abruptly', 'absent', 'absentminded', 'absolute']\n"
     ]
    }
   ],
   "source": [
    "# Se crea el vocabulario de palabras\n",
    "voc = set(data)\n",
    "voc = sorted(voc)\n",
    "print(len(voc))\n",
    "print(voc[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hablar de :\n",
    "diferentes metodos de encoding a nivel general (cuales no se van a usar)\n",
    "datos a utilizar\n",
    "preprocesamiento\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot Encoder\n",
    "El One Hot Encoder es una evolucion del indicator encoding. Este ultimo codifica cada palabra en una nueva variable hasta obtener N-1 columnas, siendo N el tamaño del vocabulario. El indicator encoding eliminaba una columna dado que las categorias son mutuamente excluyentes, por lo tanto el tener N variables podria romper el supuesto de independencia entre las palabras que se quieren modelar. Actualmente, esto no es necesario dado que que los modelos modernos generalmente incorporan metodos de regularizacion, disminuyendo la importancia de variables linealmente correlaccionadas.\n",
    "\n",
    "A partir de lo anterior, surge el one Hot Encoder el cual es uno de los mas usados entre los metodos de conteo, el cual crea un vector para cada palabra y dentro de ese vector se asigna el valor de 1 para representar la presencia de la palabra dentro de una posicion especifica del vocabulario\n",
    "\n",
    "$$abandono = [0, 0, 0, 0, 1, 0, 0 ..... 0, 0, 0, 0]_{n}$$\n",
    "\n",
    "Para el ejemplo anterior se muestra la representacion vectorial de la palabra **abandono**, la cual tiene un valor de 1 en la posicion 4 (tomando como posicion inicial el 0). Es decir, dentro del vocabulario, **abandono** se encuentra en la posicion 4. Asimimo, cuando se agregan todos los vectores de las palabras del vocabulario se obtiene una matriz tal y como se muestra a continuacion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0_.</th>\n",
       "      <th>x0_aachen</th>\n",
       "      <th>x0_aagje</th>\n",
       "      <th>x0_aah</th>\n",
       "      <th>x0_abandon</th>\n",
       "      <th>x0_abandoned</th>\n",
       "      <th>x0_abduction</th>\n",
       "      <th>x0_aber</th>\n",
       "      <th>x0_abide</th>\n",
       "      <th>x0_ability</th>\n",
       "      <th>...</th>\n",
       "      <th>x0_zealand</th>\n",
       "      <th>x0_zero</th>\n",
       "      <th>x0_zeus</th>\n",
       "      <th>x0_zhlobin</th>\n",
       "      <th>x0_zionist</th>\n",
       "      <th>x0_zipper</th>\n",
       "      <th>x0_zone</th>\n",
       "      <th>x0_zookeeper</th>\n",
       "      <th>x0_zu</th>\n",
       "      <th>x0_zweite</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7087 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   x0_.  x0_aachen  x0_aagje  x0_aah  x0_abandon  x0_abandoned  x0_abduction  \\\n",
       "0   1.0        0.0       0.0     0.0         0.0           0.0           0.0   \n",
       "1   0.0        1.0       0.0     0.0         0.0           0.0           0.0   \n",
       "2   0.0        0.0       1.0     0.0         0.0           0.0           0.0   \n",
       "3   0.0        0.0       0.0     1.0         0.0           0.0           0.0   \n",
       "4   0.0        0.0       0.0     0.0         1.0           0.0           0.0   \n",
       "\n",
       "   x0_aber  x0_abide  x0_ability  ...  x0_zealand  x0_zero  x0_zeus  \\\n",
       "0      0.0       0.0         0.0  ...         0.0      0.0      0.0   \n",
       "1      0.0       0.0         0.0  ...         0.0      0.0      0.0   \n",
       "2      0.0       0.0         0.0  ...         0.0      0.0      0.0   \n",
       "3      0.0       0.0         0.0  ...         0.0      0.0      0.0   \n",
       "4      0.0       0.0         0.0  ...         0.0      0.0      0.0   \n",
       "\n",
       "   x0_zhlobin  x0_zionist  x0_zipper  x0_zone  x0_zookeeper  x0_zu  x0_zweite  \n",
       "0         0.0         0.0        0.0      0.0           0.0    0.0        0.0  \n",
       "1         0.0         0.0        0.0      0.0           0.0    0.0        0.0  \n",
       "2         0.0         0.0        0.0      0.0           0.0    0.0        0.0  \n",
       "3         0.0         0.0        0.0      0.0           0.0    0.0        0.0  \n",
       "4         0.0         0.0        0.0      0.0           0.0    0.0        0.0  \n",
       "\n",
       "[5 rows x 7087 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aplicando OHE\n",
    "OHE = OneHotEncoder()\n",
    "OHE.fit(np.reshape(voc, (-1, 1)))\n",
    "embeddingOHE = pd.DataFrame(OHE.transform(np.reshape(voc, (-1, 1))).toarray(), columns=OHE.get_feature_names_out())\n",
    "embeddingOHE.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tal como se muestra en la tabla, se crea una matriz simetrica de 7087x7087. El hecho de que sea simetrica indica que el vector que representa cada palabra puede obtenerse al extraer una fila o columna de la matriz. Por ejemplo, para la palabra abandono se puede extraer el vector para ambos componentes y compararlos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector abandono por columnas: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "Vector abandono por filas: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Extrayendo 10 primeros elementos vector palabra abandon\n",
    "print(\"Vector abandono por columnas:\", np.array(embeddingOHE['x0_abandon'])[:10])\n",
    "print(\"Vector abandono por filas:\", np.array(embeddingOHE.iloc[4, :])[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se presentó anteriormente, una de las principales ventajas del OHE es su facilidad de entendimiento y por ende aplicabilidad a diferentes tipos de problemas tabulares. Sin embargo, una de las principales desventajas es la longitud para representar cada palabra: en el caso anterior se obtienen vectores de longitud 7087, que en otros problemas puede ser mayor.\n",
    "\n",
    "Otro aspecto negativo relacionado con el anterior es su dependencia al preprocesamiento, es decir, si bien esta etapa es muy importante en cualquier problema de ciencia de datos, este metodo depende al 100% de el dado que si no se hace una correcta eliminacion de stopwords o caracteres extraños, estos haran parte del vocabulario incrementando aun mas la longitud del vector.\n",
    "\n",
    "Finalmente, la debilidad mas grande del OHE es la poca o nula semantica de los vectores en referencia a sus palabras. Por ejemplo, las palabras **abandon** y **abandoned** son la misma palabra pero en tiempos linguisticos distintos (presente y pasado respectivamente). Aunque se encuentren en tiempos distintos, estas deberian tener cierta relacion dado que linguisticamente representan conceptos similares. Tal similaridad se evalua haciendo uso de la distancia coseno:\n",
    "\n",
    "$$cosineSimilarity = \\frac{A\\cdot{B}}{\\lVert A \\lVert \\lVert B \\lVert }$$\n",
    "\n",
    "Donde A y B son los vectores que representan las palabras, en este caso, vectores construidos mediante OHE Encoding. Si la similaridad es 1 significa que estas palabras son iguales, si es -1 indica que son opuestas (antonimos) y si es cero indica que no poseen ninguna clase de relacion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculando similaridad\n",
    "cosine_similarity(np.array(embeddingOHE['x0_abandon']).reshape(1, -1),\n",
    "                  np.array(embeddingOHE['x0_abandoned']).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa que para estas dos palabras, la similaridad de su representacion es 0, por lo tanto cualquier algoritmo asumiria que son palabras sin ninguna relacion, lo cual no es tan cierto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
